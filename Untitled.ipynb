{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802cd389-59b3-41c0-88be-22473e238650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebed0fcc-d70d-43f1-8987-8d495f5f2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "OLLAMA_URL = \"http://localhost:11434/api\"\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "# Use your own GitHub token for higher rate limits\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7268682-206e-47ae-8470-264a85b44e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_ollama(\n",
    "    prompt: str, \n",
    "    model: str = \"llama3\", \n",
    "    system_prompt: str = \"\", \n",
    "    max_tokens: int = 1000\n",
    ") -> str:\n",
    "    \"\"\"Query Ollama API with the given prompt.\"\"\"\n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": system_prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"num_predict\": max_tokens}\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{OLLAMA_URL}/generate\", headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying Ollama: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_embeddings(text: str, model: str = \"llama3\") -> List[float]:\n",
    "    \"\"\"Get embeddings from Ollama.\"\"\"\n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        data = {\"model\": model, \"prompt\": text}\n",
    "        \n",
    "        response = requests.post(f\"{OLLAMA_URL}/embeddings\", headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"embedding\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting embeddings: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338e683e-b7d2-4552-a9e3-ba9f0af17b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_project(project_description: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Break down project into subprojects using LLM.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical architect. Break down the given project into logical subprojects.\n",
    "    For each subproject, provide:\n",
    "    1. Name\n",
    "    2. Description\n",
    "    3. Technical requirements \n",
    "    4. Dependencies on other subprojects\n",
    "    \n",
    "    Format your response as a JSON array of objects with the structure:\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"tech_requirements\": [\"string\"],\n",
    "            \"dependencies\": [\"string\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    Keep your response concise and ONLY return the JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"Project description: {project_description}\\n\\nBreak this down into logical subprojects:\"\n",
    "    \n",
    "    response = query_ollama(prompt, system_prompt=system_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Find the JSON content in the response\n",
    "        json_content = response\n",
    "        if \"```json\" in response:\n",
    "            json_content = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response:\n",
    "            json_content = response.split(\"```\")[1].strip()\n",
    "            \n",
    "        subprojects = json.loads(json_content)\n",
    "        \n",
    "        # Generate embeddings for each subproject\n",
    "        for subproject in subprojects:\n",
    "            subproject_text = f\"{subproject['name']} {subproject['description']} {' '.join(subproject['tech_requirements'])}\"\n",
    "            subproject[\"embedding\"] = get_embeddings(subproject_text)\n",
    "            \n",
    "        return subprojects\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing LLM response for project decomposition: {e}\")\n",
    "        logger.debug(f\"Response: {response}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f1d34d-07f3-4f29-8cdc-7dbc17d4ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_github_repos(query: str, n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search GitHub for repositories matching the query.\"\"\"\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"stars\",\n",
    "        \"order\": \"desc\",\n",
    "        \"per_page\": n\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{GITHUB_API_URL}/search/repositories\", headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        repos = response.json().get(\"items\", [])\n",
    "        \n",
    "        result = []\n",
    "        for repo in repos:\n",
    "            # Get additional details about each repo\n",
    "            languages_url = repo.get(\"languages_url\")\n",
    "            languages = []\n",
    "            if languages_url:\n",
    "                lang_response = requests.get(languages_url, headers=headers)\n",
    "                if lang_response.status_code == 200:\n",
    "                    languages = list(lang_response.json().keys())\n",
    "            \n",
    "            readme_content = get_readme(repo[\"full_name\"], headers)\n",
    "            \n",
    "            repo_info = {\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"full_name\": repo[\"full_name\"],\n",
    "                \"description\": repo[\"description\"] or \"\",\n",
    "                \"url\": repo[\"html_url\"],\n",
    "                \"stars\": repo[\"stargazers_count\"],\n",
    "                \"forks\": repo[\"forks_count\"],\n",
    "                \"languages\": languages,\n",
    "                \"license\": repo.get(\"license\", {}).get(\"spdx_id\") if repo.get(\"license\") else None,\n",
    "                \"topics\": repo.get(\"topics\", []),\n",
    "                \"readme_summary\": summarize_readme(readme_content),\n",
    "                \"embedding\": get_embeddings(f\"{repo['name']} {repo['description'] or ''} {' '.join(repo.get('topics', []))}\")\n",
    "            }\n",
    "            result.append(repo_info)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching GitHub: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_readme(repo_full_name: str, headers: Dict[str, str]) -> str:\n",
    "    \"\"\"Get README content for a repository.\"\"\"\n",
    "    try:\n",
    "        # First try to get the default README\n",
    "        response = requests.get(\n",
    "            f\"{GITHUB_API_URL}/repos/{repo_full_name}/readme\",\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json().get(\"content\", \"\")\n",
    "            encoding = response.json().get(\"encoding\", \"\")\n",
    "            \n",
    "            if encoding == \"base64\" and content:\n",
    "                import base64\n",
    "                return base64.b64decode(content).decode('utf-8', errors='replace')\n",
    "        \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting README: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def summarize_readme(readme_content: str) -> str:\n",
    "    \"\"\"Summarize README content using LLM.\"\"\"\n",
    "    if not readme_content or len(readme_content) < 100:\n",
    "        return readme_content\n",
    "    \n",
    "    # Truncate very long READMEs to avoid token limits\n",
    "    if len(readme_content) > 4000:\n",
    "        readme_content = readme_content[:4000] + \"...\"\n",
    "    \n",
    "    system_prompt = \"Summarize the following README in 2-3 sentences, focusing on what the repository does and its key features:\"\n",
    "    \n",
    "    return query_ollama(readme_content, system_prompt=system_prompt, max_tokens=200)\n",
    "\n",
    "def find_repos_for_subproject(subproject: Dict[str, Any], n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find repositories for a specific subproject.\"\"\"\n",
    "    # Construct search query from subproject details\n",
    "    search_terms = [\n",
    "        subproject[\"name\"],\n",
    "        *[req for req in subproject[\"tech_requirements\"] if len(req.split()) <= 3]  # Only include short tech requirements\n",
    "    ]\n",
    "    \n",
    "    # Add some of the most informative words from the description\n",
    "    desc_words = subproject[\"description\"].split()\n",
    "    if len(desc_words) > 5:\n",
    "        # Simple approach: take first 3 words that are longer than 3 characters\n",
    "        additional_terms = [word for word in desc_words if len(word) > 3][:3]\n",
    "        search_terms.extend(additional_terms)\n",
    "    \n",
    "    # Join terms and ensure we don't exceed GitHub's query length limits\n",
    "    query = \" \".join(search_terms)\n",
    "    if len(query) > 256:  # GitHub has query length limits\n",
    "        query = query[:256]\n",
    "    \n",
    "    return search_github_repos(query, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f09bc2-66c2-458e-9553-b43909eb6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_conflicts(subprojects: List[Dict[str, Any]], repos_by_subproject: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze potential integration conflicts between selected repositories.\"\"\"\n",
    "    conflicts = []\n",
    "    \n",
    "    # Simple language compatibility matrix (greatly simplified for POC)\n",
    "    # In a real system, this would be much more nuanced\n",
    "    compatible_languages = {\n",
    "        \"Python\": [\"Python\", \"JavaScript\", \"TypeScript\"],\n",
    "        \"JavaScript\": [\"JavaScript\", \"TypeScript\", \"Python\"],\n",
    "        \"TypeScript\": [\"TypeScript\", \"JavaScript\", \"Python\"],\n",
    "        \"Java\": [\"Java\", \"Kotlin\"],\n",
    "        \"Kotlin\": [\"Kotlin\", \"Java\"],\n",
    "        \"Go\": [\"Go\"],\n",
    "        \"Rust\": [\"Rust\"],\n",
    "        \"C#\": [\"C#\", \"F#\"],\n",
    "        \"PHP\": [\"PHP\", \"JavaScript\"]\n",
    "    }\n",
    "    \n",
    "    # Check for language conflicts\n",
    "    for i, subproject1 in enumerate(subprojects):\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            languages1 = repo1.get(\"languages\", [])\n",
    "            \n",
    "            for j, subproject2 in enumerate(subprojects):\n",
    "                if i == j:\n",
    "                    continue  # Skip self-comparison\n",
    "                    \n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if subproject2 depends on subproject1\n",
    "                if name1 not in subproject2.get(\"dependencies\", []):\n",
    "                    continue\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    languages2 = repo2.get(\"languages\", [])\n",
    "                    \n",
    "                    # Check language compatibility\n",
    "                    if languages1 and languages2:\n",
    "                        primary_lang1 = languages1[0] if languages1 else None\n",
    "                        primary_lang2 = languages2[0] if languages2 else None\n",
    "                        \n",
    "                        if (primary_lang1 and primary_lang2 and \n",
    "                            primary_lang1 not in compatible_languages.get(primary_lang2, [])):\n",
    "                            conflicts.append({\n",
    "                                \"type\": \"language_incompatibility\",\n",
    "                                \"severity\": \"high\",\n",
    "                                \"description\": f\"Language incompatibility between {name1} ({primary_lang1}) and {name2} ({primary_lang2})\",\n",
    "                                \"subprojects\": [name1, name2],\n",
    "                                \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]]\n",
    "                            })\n",
    "    \n",
    "    # Simple license compatibility check\n",
    "    incompatible_licenses = {\n",
    "        \"AGPL-3.0\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\"],  # Simplified for POC\n",
    "        \"GPL-3.0\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\"]    # Actual compatibility is more complex\n",
    "    }\n",
    "    \n",
    "    for subproject1 in subprojects:\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            license1 = repo1.get(\"license\")\n",
    "            \n",
    "            for subproject2 in subprojects:\n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 == name1 or name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    license2 = repo2.get(\"license\")\n",
    "                    \n",
    "                    if (license1 and license2 and \n",
    "                        license1 in incompatible_licenses and \n",
    "                        license2 in incompatible_licenses.get(license1, [])):\n",
    "                        conflicts.append({\n",
    "                            \"type\": \"license_incompatibility\",\n",
    "                            \"severity\": \"high\",\n",
    "                            \"description\": f\"License incompatibility between {name1} ({license1}) and {name2} ({license2})\",\n",
    "                            \"subprojects\": [name1, name2],\n",
    "                            \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]]\n",
    "                        })\n",
    "    \n",
    "    return conflicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9583319f-fc0f-4d02-a8c7-3a040b6c5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_integration_plan(\n",
    "    project_description: str,\n",
    "    subprojects: List[Dict[str, Any]],\n",
    "    selected_repos: Dict[str, Dict[str, Any]],\n",
    "    conflicts: List[Dict[str, Any]]\n",
    ") -> str:\n",
    "    \"\"\"Generate an integration plan using LLM.\"\"\"\n",
    "    # Prepare a concise context for the LLM\n",
    "    subproject_summaries = []\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repo = selected_repos.get(name, {})\n",
    "        repo_info = \"\"\n",
    "        if repo:\n",
    "            repo_info = f\"Using repo: {repo['name']} ({repo.get('url', '')}) - {repo.get('description', '')}\"\n",
    "            \n",
    "        summary = f\"- {name}: {subproject['description']}\\n  {repo_info}\"\n",
    "        subproject_summaries.append(summary)\n",
    "    \n",
    "    conflict_summaries = []\n",
    "    for conflict in conflicts:\n",
    "        summary = f\"- {conflict['description']}\"\n",
    "        conflict_summaries.append(summary)\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical integration architect. Create a practical integration plan for connecting the selected repositories.\n",
    "    Focus on:\n",
    "    1. Key integration points between components\n",
    "    2. Modifications needed for each repository\n",
    "    3. Addressing identified conflicts\n",
    "    4. A logical sequence for integration\n",
    "    \n",
    "    Keep your answer concise, practical and implementation-focused.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Project: {project_description}\n",
    "    \n",
    "    Subprojects and Selected Repositories:\n",
    "    {chr(10).join(subproject_summaries)}\n",
    "    \n",
    "    Identified Conflicts:\n",
    "    {chr(10).join(conflict_summaries) if conflict_summaries else \"No major conflicts identified.\"}\n",
    "    \n",
    "    Please create an integration plan that shows how to connect these components.\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_ollama(prompt, system_prompt=system_prompt, max_tokens=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2691850a-aec6-445e-8457-b9bb5d5cee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:29:48,669 - INFO - Starting project planning process\n",
      "2025-05-12 13:29:48,672 - INFO - Decomposing project...\n",
      "2025-05-12 13:29:48,688 - ERROR - Error querying Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea4e2f40650>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2025-05-12 13:29:48,690 - ERROR - Error parsing LLM response for project decomposition: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"Failed to decompose project\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======== Semantic Matching Functions ========\n",
    "\n",
    "def rank_repos_by_similarity(subproject: Dict[str, Any], repos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Rank repositories by semantic similarity to subproject.\"\"\"\n",
    "    if not repos or not subproject.get(\"embedding\") or len(subproject[\"embedding\"]) == 0:\n",
    "        return repos\n",
    "        \n",
    "    similarities = []\n",
    "    for repo in repos:\n",
    "        if not repo.get(\"embedding\") or len(repo[\"embedding\"]) == 0:\n",
    "            similarities.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            [subproject[\"embedding\"]], \n",
    "            [repo[\"embedding\"]]\n",
    "        )[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Sort repos by similarity\n",
    "    sorted_repos = [repo for _, repo in sorted(\n",
    "        zip(similarities, repos), \n",
    "        key=lambda pair: pair[0], \n",
    "        reverse=True\n",
    "    )]\n",
    "    \n",
    "    return sorted_repos\n",
    "\n",
    "# ======== Main Execution Function ========\n",
    "\n",
    "def create_project_plan(project_description: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create a complete project plan from description.\"\"\"\n",
    "    logger.info(\"Starting project planning process\")\n",
    "    \n",
    "    # Step 1: Decompose project into subprojects\n",
    "    logger.info(\"Decomposing project...\")\n",
    "    subprojects = decompose_project(project_description)\n",
    "    if not subprojects:\n",
    "        return {\"error\": \"Failed to decompose project\"}\n",
    "    \n",
    "    # Step 2: Find repositories for each subproject\n",
    "    logger.info(\"Searching repositories...\")\n",
    "    repos_by_subproject = {}\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repos = find_repos_for_subproject(subproject)\n",
    "        if repos:\n",
    "            # Rank repos by semantic similarity\n",
    "            ranked_repos = rank_repos_by_similarity(subproject, repos)\n",
    "            repos_by_subproject[name] = ranked_repos\n",
    "    \n",
    "    # Step 3: Select best repository for each subproject (simplified for POC)\n",
    "    selected_repos = {}\n",
    "    for name, repos in repos_by_subproject.items():\n",
    "        if repos:\n",
    "            selected_repos[name] = repos[0]  # Select highest ranked repo\n",
    "    \n",
    "    # Step 4: Analyze conflicts\n",
    "    logger.info(\"Analyzing potential conflicts...\")\n",
    "    conflicts = analyze_conflicts(subprojects, repos_by_subproject)\n",
    "    \n",
    "    # Step 5: Generate integration plan\n",
    "    logger.info(\"Generating integration plan...\")\n",
    "    integration_plan = generate_integration_plan(\n",
    "        project_description, \n",
    "        subprojects, \n",
    "        selected_repos,\n",
    "        conflicts\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"project_description\": project_description,\n",
    "        \"subprojects\": subprojects,\n",
    "        \"selected_repositories\": selected_repos,\n",
    "        \"alternative_repositories\": repos_by_subproject,\n",
    "        \"conflicts\": conflicts,\n",
    "        \"integration_plan\": integration_plan\n",
    "    }\n",
    "\n",
    "# ======== Demo Usage ========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example project description\n",
    "    project_description = \"\"\"\n",
    "    Build a web application that allows users to upload images, \n",
    "    apply ML-based filters, and share them on social media.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the planner\n",
    "    plan = create_project_plan(project_description)\n",
    "    \n",
    "    # Output the plan (could be saved to file or displayed in a UI)\n",
    "    print(json.dumps(plan, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3090a5f5-1dfc-407e-bc32-d7eb7262c893",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[32m     22\u001b[39m GITHUB_API_URL = \u001b[33m\"\u001b[39m\u001b[33mhttps://api.github.com\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Project Planning Tool: Finds repositories and creates integration plans for software projects.\n",
    "Provides automated decomposition of projects and repository matching using HuggingFace Transformers.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "import re\n",
    "import base64\n",
    "from functools import lru_cache\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"\")\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, LOG_LEVEL),\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"project_planner\")\n",
    "\n",
    "# ======== Model Setup ========\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_embedding_model():\n",
    "    \"\"\"Load and cache the embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use Sentence Transformers for high-quality embeddings\n",
    "        logger.info(\"Loading embedding model...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading embedding model: {e}\")\n",
    "        return None\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_llm_model():\n",
    "    \"\"\"Load and cache the language model for text generation.\"\"\"\n",
    "    try:\n",
    "        # Use a small but capable model for text generation\n",
    "        logger.info(\"Loading language model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        model = AutoModel.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading language model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ======== Embedding Functions ========\n",
    "\n",
    "def get_embeddings(text: str) -> List[float]:\n",
    "    \"\"\"Get embeddings for text using SentenceTransformers.\"\"\"\n",
    "    try:\n",
    "        model = get_embedding_model()\n",
    "        if model is None:\n",
    "            return []\n",
    "            \n",
    "        # Encode the text to get embeddings\n",
    "        embedding = model.encode(text)\n",
    "        return embedding.tolist()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "def compute_similarity(embedding1: List[float], embedding2: List[float]) -> float:\n",
    "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "    if not embedding1 or not embedding2:\n",
    "        return 0.0\n",
    "        \n",
    "    try:\n",
    "        return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing similarity: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# ======== LLM Query Function ========\n",
    "\n",
    "def query_huggingface(\n",
    "    prompt: str, \n",
    "    system_prompt: str = \"\", \n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Query local HuggingFace model with the given prompt.\"\"\"\n",
    "    try:\n",
    "        tokenizer, model = get_llm_model()\n",
    "        if tokenizer is None or model is None:\n",
    "            return \"Model loading failed. Please check logs.\"\n",
    "            \n",
    "        # Fall back to Hugging Face Inference API if available\n",
    "        if os.getenv(\"HF_API_TOKEN\"):\n",
    "            return query_hf_inference_api(prompt, system_prompt, max_tokens, temperature)\n",
    "            \n",
    "        # Format input for model\n",
    "        if system_prompt:\n",
    "            input_text = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        else:\n",
    "            input_text = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            # Move to GPU if available\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate text\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        # Decode the generated text\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract assistant's response\n",
    "        match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)(?:<\\|im_end\\|>|$)\", output, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return output.split(\"<|im_start|>assistant\\n\")[-1].strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying local model: {e}\")\n",
    "        fallback_message = f\"Error using local model: {str(e)}. Attempting to use Hugging Face Inference API...\"\n",
    "        logger.info(fallback_message)\n",
    "        \n",
    "        # Try using the Hugging Face Inference API as fallback\n",
    "        if os.getenv(\"HF_API_TOKEN\"):\n",
    "            return query_hf_inference_api(prompt, system_prompt, max_tokens, temperature)\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def query_hf_inference_api(\n",
    "    prompt: str,\n",
    "    system_prompt: str = \"\",\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Query Hugging Face Inference API as a fallback.\"\"\"\n",
    "    try:\n",
    "        API_URL = \"https://api-inference.huggingface.co/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        headers = {\"Authorization\": f\"Bearer {os.getenv('HF_API_TOKEN')}\"}\n",
    "        \n",
    "        # Format the prompt\n",
    "        if system_prompt:\n",
    "            payload = {\n",
    "                \"inputs\": f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            payload = {\n",
    "                \"inputs\": f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[0].get(\"generated_text\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error using HF Inference API: {e}\")\n",
    "        return f\"Error with Inference API: {str(e)}\"\n",
    "\n",
    "# ======== Project Decomposition ========\n",
    "\n",
    "def decompose_project(project_description: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Break down project into subprojects using LLM.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical architect. Break down the given project into logical subprojects.\n",
    "    For each subproject, provide:\n",
    "    1. Name\n",
    "    2. Description\n",
    "    3. Technical requirements \n",
    "    4. Dependencies on other subprojects\n",
    "    \n",
    "    Format your response as a JSON array of objects with the structure:\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"tech_requirements\": [\"string\"],\n",
    "            \"dependencies\": [\"string\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    Keep your response concise and ONLY return the JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"Project description: {project_description}\\n\\nBreak this down into logical subprojects:\"\n",
    "    \n",
    "    response = query_huggingface(prompt, system_prompt=system_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Find the JSON content in the response\n",
    "        json_content = response\n",
    "        if \"```json\" in response:\n",
    "            json_content = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response:\n",
    "            json_content = response.split(\"```\")[1].strip()\n",
    "            \n",
    "        subprojects = json.loads(json_content)\n",
    "        \n",
    "        # Generate embeddings for each subproject\n",
    "        for subproject in subprojects:\n",
    "            subproject_text = f\"{subproject['name']} {subproject['description']} {' '.join(subproject['tech_requirements'])}\"\n",
    "            subproject[\"embedding\"] = get_embeddings(subproject_text)\n",
    "            \n",
    "        return subprojects\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing LLM response for project decomposition: {e}\")\n",
    "        logger.debug(f\"Response: {response}\")\n",
    "        return []\n",
    "\n",
    "# ======== GitHub Repository Search ========\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def search_github_repos(query: str, n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search GitHub for repositories matching the query, with caching for efficiency.\"\"\"\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"stars\",\n",
    "        \"order\": \"desc\",\n",
    "        \"per_page\": n\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Searching GitHub for: {query}\")\n",
    "        response = requests.get(f\"{GITHUB_API_URL}/search/repositories\", headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        repos = response.json().get(\"items\", [])\n",
    "        \n",
    "        result = []\n",
    "        for repo in repos:\n",
    "            # Get additional details about each repo\n",
    "            languages_url = repo.get(\"languages_url\")\n",
    "            languages = []\n",
    "            if languages_url:\n",
    "                lang_response = requests.get(languages_url, headers=headers)\n",
    "                if lang_response.status_code == 200:\n",
    "                    languages = list(lang_response.json().keys())\n",
    "            \n",
    "            # Get README content asynchronously to improve performance\n",
    "            readme_content = get_readme(repo[\"full_name\"], headers)\n",
    "            \n",
    "            # Create embedding for the repository\n",
    "            embedding_text = f\"{repo['name']} {repo['description'] or ''} {' '.join(repo.get('topics', []))}\"\n",
    "            \n",
    "            repo_info = {\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"full_name\": repo[\"full_name\"],\n",
    "                \"description\": repo[\"description\"] or \"\",\n",
    "                \"url\": repo[\"html_url\"],\n",
    "                \"stars\": repo[\"stargazers_count\"],\n",
    "                \"forks\": repo[\"forks_count\"],\n",
    "                \"languages\": languages,\n",
    "                \"license\": repo.get(\"license\", {}).get(\"spdx_id\") if repo.get(\"license\") else None,\n",
    "                \"topics\": repo.get(\"topics\", []),\n",
    "                \"readme_summary\": summarize_readme(readme_content),\n",
    "                \"embedding\": get_embeddings(embedding_text)\n",
    "            }\n",
    "            result.append(repo_info)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching GitHub: {e}\")\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=64)\n",
    "def get_readme(repo_full_name: str, headers: Dict[str, str]) -> str:\n",
    "    \"\"\"Get README content for a repository with caching.\"\"\"\n",
    "    try:\n",
    "        # Try to get the default README\n",
    "        response = requests.get(\n",
    "            f\"{GITHUB_API_URL}/repos/{repo_full_name}/readme\",\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json().get(\"content\", \"\")\n",
    "            encoding = response.json().get(\"encoding\", \"\")\n",
    "            \n",
    "            if encoding == \"base64\" and content:\n",
    "                return base64.b64decode(content).decode('utf-8', errors='replace')\n",
    "        \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting README: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def summarize_readme(readme_content: str) -> str:\n",
    "    \"\"\"Summarize README content using LLM.\"\"\"\n",
    "    if not readme_content or len(readme_content) < 100:\n",
    "        return readme_content\n",
    "    \n",
    "    # Truncate very long READMEs to avoid token limits\n",
    "    if len(readme_content) > 4000:\n",
    "        readme_content = readme_content[:4000] + \"...\"\n",
    "    \n",
    "    system_prompt = \"Summarize the following README in 2-3 sentences, focusing on what the repository does and its key features:\"\n",
    "    \n",
    "    return query_huggingface(readme_content, system_prompt=system_prompt, max_tokens=200)\n",
    "\n",
    "def find_repos_for_subproject(subproject: Dict[str, Any], n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find repositories for a specific subproject, with efficient search query construction.\"\"\"\n",
    "    # Construct optimized search query from subproject details\n",
    "    search_terms = [\n",
    "        subproject[\"name\"],\n",
    "        *[req for req in subproject[\"tech_requirements\"] if len(req.split()) <= 3]  # Only include short tech requirements\n",
    "    ]\n",
    "    \n",
    "    # Use NLP to extract key terms from description\n",
    "    if subproject[\"description\"]:\n",
    "        # Simple keyword extraction - in production you might use NLP techniques\n",
    "        stop_words = {\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \"and\", \"or\", \"of\"}\n",
    "        desc_words = [word.lower() for word in re.findall(r'\\w+', subproject[\"description\"]) \n",
    "                      if word.lower() not in stop_words and len(word) > 3]\n",
    "        \n",
    "        # Take most important keywords (longer words tend to be more specific)\n",
    "        desc_words.sort(key=len, reverse=True)\n",
    "        search_terms.extend(desc_words[:3])\n",
    "    \n",
    "    # Make query unique by removing duplicates while preserving order\n",
    "    seen = set()\n",
    "    query_terms = [term for term in search_terms if not (term.lower() in seen or seen.add(term.lower()))]\n",
    "    \n",
    "    # Join terms and ensure we don't exceed GitHub's query length limits\n",
    "    query = \" \".join(query_terms)\n",
    "    if len(query) > 256:  # GitHub has query length limits\n",
    "        query = query[:256]\n",
    "    \n",
    "    return search_github_repos(query, n)\n",
    "\n",
    "# ======== Conflict Analysis ========\n",
    "\n",
    "def analyze_conflicts(subprojects: List[Dict[str, Any]], repos_by_subproject: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze potential integration conflicts between selected repositories.\"\"\"\n",
    "    conflicts = []\n",
    "    \n",
    "    # More comprehensive language compatibility matrix\n",
    "    compatible_languages = {\n",
    "        \"Python\": [\"Python\", \"JavaScript\", \"TypeScript\", \"C\", \"C++\", \"Rust\"],\n",
    "        \"JavaScript\": [\"JavaScript\", \"TypeScript\", \"Python\", \"HTML\", \"CSS\", \"Ruby\"],\n",
    "        \"TypeScript\": [\"TypeScript\", \"JavaScript\", \"Python\", \"HTML\", \"CSS\"],\n",
    "        \"Java\": [\"Java\", \"Kotlin\", \"Scala\", \"Groovy\"],\n",
    "        \"Kotlin\": [\"Kotlin\", \"Java\", \"Scala\"],\n",
    "        \"Go\": [\"Go\", \"C\", \"Rust\"],\n",
    "        \"Rust\": [\"Rust\", \"C\", \"C++\", \"Go\"],\n",
    "        \"C#\": [\"C#\", \"F#\", \"Visual Basic\", \"JavaScript\"],\n",
    "        \"PHP\": [\"PHP\", \"JavaScript\", \"HTML\", \"CSS\"],\n",
    "        \"Ruby\": [\"Ruby\", \"JavaScript\", \"HTML\", \"CSS\"],\n",
    "        \"Swift\": [\"Swift\", \"Objective-C\", \"C\"],\n",
    "        \"C++\": [\"C++\", \"C\", \"Rust\", \"Python\"],\n",
    "        \"C\": [\"C\", \"C++\", \"Rust\", \"Assembly\"]\n",
    "    }\n",
    "    \n",
    "    # More comprehensive license compatibility matrix\n",
    "    # This is simplified - real license compatibility is complex and should be evaluated by legal professionals\n",
    "    license_compatibility = {\n",
    "        \"AGPL-3.0\": {\n",
    "            \"incompatible\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\"],\n",
    "            \"compatible\": [\"GPL-3.0\", \"LGPL-3.0\"]\n",
    "        },\n",
    "        \"GPL-3.0\": {\n",
    "            \"incompatible\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\"],\n",
    "            \"compatible\": [\"LGPL-3.0\", \"AGPL-3.0\"]\n",
    "        },\n",
    "        \"LGPL-3.0\": {\n",
    "            \"incompatible\": [],\n",
    "            \"compatible\": [\"GPL-3.0\", \"AGPL-3.0\", \"MIT\", \"Apache-2.0\", \"BSD-3-Clause\"]\n",
    "        },\n",
    "        \"MIT\": {\n",
    "            \"incompatible\": [\"AGPL-3.0\", \"GPL-3.0\"],\n",
    "            \"compatible\": [\"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\", \"LGPL-3.0\"]\n",
    "        },\n",
    "        \"Apache-2.0\": {\n",
    "            \"incompatible\": [\"AGPL-3.0\", \"GPL-3.0\"],\n",
    "            \"compatible\": [\"MIT\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\", \"LGPL-3.0\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check for language conflicts\n",
    "    for i, subproject1 in enumerate(subprojects):\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            languages1 = repo1.get(\"languages\", [])\n",
    "            primary_lang1 = languages1[0] if languages1 else None\n",
    "            \n",
    "            for j, subproject2 in enumerate(subprojects):\n",
    "                if i == j:\n",
    "                    continue  # Skip self-comparison\n",
    "                    \n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if subproject2 depends on subproject1 or vice versa\n",
    "                has_dependency = (name1 in subproject2.get(\"dependencies\", []) or\n",
    "                                 name2 in subproject1.get(\"dependencies\", []))\n",
    "                \n",
    "                if not has_dependency:\n",
    "                    continue  # Only check conflicts for dependent components\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    languages2 = repo2.get(\"languages\", [])\n",
    "                    primary_lang2 = languages2[0] if languages2 else None\n",
    "                    \n",
    "                    # Check language compatibility\n",
    "                    if (primary_lang1 and primary_lang2 and \n",
    "                        primary_lang1 in compatible_languages and\n",
    "                        primary_lang2 not in compatible_languages.get(primary_lang1, [])):\n",
    "                        conflicts.append({\n",
    "                            \"type\": \"language_incompatibility\",\n",
    "                            \"severity\": \"high\",\n",
    "                            \"description\": f\"Language incompatibility between {name1} ({primary_lang1}) and {name2} ({primary_lang2})\",\n",
    "                            \"subprojects\": [name1, name2],\n",
    "                            \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]],\n",
    "                            \"mitigation\": f\"Consider using language bindings, APIs, or microservices to integrate {primary_lang1} with {primary_lang2}\"\n",
    "                        })\n",
    "    \n",
    "    # Check for license conflicts\n",
    "    for subproject1 in subprojects:\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            license1 = repo1.get(\"license\")\n",
    "            \n",
    "            for subproject2 in subprojects:\n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 == name1 or name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    license2 = repo2.get(\"license\")\n",
    "                    \n",
    "                    # Check license compatibility\n",
    "                    if (license1 and license2 and \n",
    "                        license1 in license_compatibility and\n",
    "                        license2 in license_compatibility.get(license1, {}).get(\"incompatible\", [])):\n",
    "                        conflicts.append({\n",
    "                            \"type\": \"license_incompatibility\",\n",
    "                            \"severity\": \"high\",\n",
    "                            \"description\": f\"License incompatibility between {name1} ({license1}) and {name2} ({license2})\",\n",
    "                            \"subprojects\": [name1, name2],\n",
    "                            \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]],\n",
    "                            \"mitigation\": \"Consider finding alternative repositories with compatible licenses or seeking legal advice\"\n",
    "                        })\n",
    "    \n",
    "    return conflicts\n",
    "\n",
    "# ======== Integration Planning ========\n",
    "\n",
    "def generate_integration_plan(\n",
    "    project_description: str,\n",
    "    subprojects: List[Dict[str, Any]],\n",
    "    selected_repos: Dict[str, Dict[str, Any]],\n",
    "    conflicts: List[Dict[str, Any]]\n",
    ") -> str:\n",
    "    \"\"\"Generate an integration plan using LLM with improved prompting.\"\"\"\n",
    "    # Prepare a structured context for the LLM\n",
    "    subproject_summaries = []\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repo = selected_repos.get(name, {})\n",
    "        \n",
    "        # Build a comprehensive description of the subproject and selected repo\n",
    "        summary = {\n",
    "            \"name\": name,\n",
    "            \"description\": subproject[\"description\"],\n",
    "            \"tech_requirements\": subproject[\"tech_requirements\"],\n",
    "            \"dependencies\": subproject.get(\"dependencies\", [])\n",
    "        }\n",
    "        \n",
    "        if repo:\n",
    "            summary[\"repository\"] = {\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"full_name\": repo[\"full_name\"],\n",
    "                \"url\": repo.get(\"url\", \"\"),\n",
    "                \"description\": repo.get(\"description\", \"\"),\n",
    "                \"primary_language\": repo.get(\"languages\", [\"\"])[0] if repo.get(\"languages\") else \"\",\n",
    "                \"stars\": repo.get(\"stars\", 0)\n",
    "            }\n",
    "            \n",
    "        subproject_summaries.append(summary)\n",
    "    \n",
    "    # Format conflict information\n",
    "    conflict_details = []\n",
    "    for conflict in conflicts:\n",
    "        conflict_details.append({\n",
    "            \"type\": conflict[\"type\"],\n",
    "            \"description\": conflict[\"description\"],\n",
    "            \"affected_components\": conflict[\"subprojects\"],\n",
    "            \"mitigation\": conflict.get(\"mitigation\", \"Needs mitigation strategy\")\n",
    "        })\n",
    "    \n",
    "    # Create structured input for the LLM\n",
    "    plan_input = {\n",
    "        \"project_description\": project_description,\n",
    "        \"subprojects\": subproject_summaries,\n",
    "        \"conflicts\": conflict_details\n",
    "    }\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical integration architect with expertise in software development and system design.\n",
    "    Create a practical integration plan for connecting the selected repositories into a cohesive system.\n",
    "    \n",
    "    Your plan should include:\n",
    "    1. Architecture overview - how components will fit together\n",
    "    2. Key integration points between components\n",
    "    3. Specific modifications needed for each repository\n",
    "    4. Step-by-step implementation plan\n",
    "    5. Solutions for the identified conflicts\n",
    "    \n",
    "    Focus on practical implementation details rather than theory.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    I need an integration plan for the following software project:\n",
    "    \n",
    "    {json.dumps(plan_input, indent=2)}\n",
    "    \n",
    "    Please provide a comprehensive integration plan that covers architecture, \n",
    "    integration points, necessary modifications, and conflict resolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_huggingface(prompt, system_prompt=system_prompt, max_tokens=1500)\n",
    "\n",
    "# ======== Semantic Matching Functions ========\n",
    "\n",
    "def rank_repos_by_similarity(subproject: Dict[str, Any], repos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Rank repositories by semantic similarity to subproject with improved algorithm.\"\"\"\n",
    "    if not repos or not subproject.get(\"embedding\") or len(subproject[\"embedding\"]) == 0:\n",
    "        return repos\n",
    "        \n",
    "    # Calculate multiple similarity factors\n",
    "    results = []\n",
    "    for repo in repos:\n",
    "        if not repo.get(\"embedding\") or len(repo[\"embedding\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        semantic_similarity = compute_similarity(subproject[\"embedding\"], repo[\"embedding\"])\n",
    "        \n",
    "        # Calculate tech requirement match score \n",
    "        tech_match_score = 0\n",
    "        if repo.get(\"topics\") and subproject.get(\"tech_requirements\"):\n",
    "            # Case-insensitive matching of tech requirements with repo topics\n",
    "            repo_topics_lower = [topic.lower() for topic in repo[\"topics\"]]\n",
    "            matched_techs = sum(1 for req in subproject[\"tech_requirements\"] \n",
    "                               if any(req.lower() in topic for topic in repo_topics_lower))\n",
    "            tech_match_score = matched_techs / len(subproject[\"tech_requirements\"]) if subproject[\"tech_requirements\"] else 0\n",
    "        \n",
    "        # Factor in repository popularity (stars) as a quality signal\n",
    "        popularity_score = min(1.0, repo.get(\"stars\", 0) / 10000)  # Cap at 10k stars for normalization\n",
    "        \n",
    "        # Combined score with weights \n",
    "        combined_score = (\n",
    "            0.5 * semantic_similarity +  # Semantic similarity is most important\n",
    "            0.3 * tech_match_score +     # Technical match is important\n",
    "            0.2 * popularity_score       # Popularity provides some quality assurance\n",
    "        )\n",
    "        \n",
    "        results.append((combined_score, repo))\n",
    "    \n",
    "    # Sort by combined score\n",
    "    results.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Return sorted repositories\n",
    "    return [repo for _, repo in results]\n",
    "\n",
    "# ======== Main Execution Function ========\n",
    "\n",
    "def create_project_plan(project_description: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create a complete project plan from description with improved execution flow.\"\"\"\n",
    "    logger.info(\"Starting project planning process\")\n",
    "    \n",
    "    # Step 1: Decompose project into subprojects\n",
    "    logger.info(\"Decomposing project...\")\n",
    "    subprojects = decompose_project(project_description)\n",
    "    if not subprojects:\n",
    "        return {\"error\": \"Failed to decompose project\"}\n",
    "    \n",
    "    # Step 2: Find repositories for each subproject\n",
    "    logger.info(\"Searching repositories...\")\n",
    "    repos_by_subproject = {}\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repos = find_repos_for_subproject(subproject, n=5)  # Get top 5 repos\n",
    "        if repos:\n",
    "            # Rank repos by similarity\n",
    "            ranked_repos = rank_repos_by_similarity(subproject, repos)\n",
    "            repos_by_subproject[name] = ranked_repos\n",
    "    \n",
    "    # Step 3: Select best repository for each subproject\n",
    "    selected_repos = {}\n",
    "    for name, repos in repos_by_subproject.items():\n",
    "        if repos:\n",
    "            selected_repos[name] = repos[0]  # Select highest ranked repo\n",
    "    \n",
    "    # Step 4: Analyze conflicts\n",
    "    logger.info(\"Analyzing potential conflicts...\")\n",
    "    conflicts = analyze_conflicts(subprojects, repos_by_subproject)\n",
    "    \n",
    "    # Step 5: Generate integration plan\n",
    "    logger.info(\"Generating integration plan...\")\n",
    "    integration_plan = generate_integration_plan(\n",
    "        project_description, \n",
    "        subprojects, \n",
    "        selected_repos,\n",
    "        conflicts\n",
    "    )\n",
    "    \n",
    "    # Build complete result\n",
    "    return {\n",
    "        \"project_description\": project_description,\n",
    "        \"subprojects\": subprojects,\n",
    "        \"selected_repositories\": selected_repos,\n",
    "        \"alternative_repositories\": repos_by_subproject,\n",
    "        \"conflicts\": conflicts,\n",
    "        \"integration_plan\": integration_plan,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "\n",
    "# ======== Demo Usage ========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example project description\n",
    "    project_description = \"\"\"\n",
    "    Build a web application that allows users to upload images, \n",
    "    apply ML-based filters, and share them on social media.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the planner\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c64e021-df60-4562-b120-eebc68a5f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:42:50,811 - INFO - Starting project planning process\n",
      "2025-05-12 13:42:50,816 - INFO - Decomposing project...\n",
      "2025-05-12 13:42:50,824 - ERROR - Error querying Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea41276a750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2025-05-12 13:42:50,830 - ERROR - Error parsing LLM response for project decomposition: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"Failed to decompose project\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "plan = create_project_plan(project_description)\n",
    "print(json.dumps(plan, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d1b8b-8cce-4e15-a904-1dace25866a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
