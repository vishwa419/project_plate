{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd5c062-8efa-4025-9137-41b5697c3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project Planning Tool: Finds repositories and creates integration plans for software projects.\n",
    "Provides automated decomposition of projects and repository matching using HuggingFace Transformers.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "import re\n",
    "import base64\n",
    "from functools import lru_cache\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "GITHUB_API_URL = \"https://api.github.com\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"\")\n",
    "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, LOG_LEVEL),\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"project_planner\")\n",
    "\n",
    "# ======== Model Setup ========\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_embedding_model():\n",
    "    \"\"\"Load and cache the embedding model.\"\"\"\n",
    "    try:\n",
    "        # Use Sentence Transformers for high-quality embeddings\n",
    "        logger.info(\"Loading embedding model...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading embedding model: {e}\")\n",
    "        return None\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_llm_model():\n",
    "    \"\"\"Load and cache the language model for text generation.\"\"\"\n",
    "    try:\n",
    "        # Use a small but capable model for text generation\n",
    "        logger.info(\"Loading language model...\")\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading language model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ======== Embedding Functions ========\n",
    "\n",
    "def get_embeddings(text: str) -> List[float]:\n",
    "    \"\"\"Get embeddings for text using SentenceTransformers.\"\"\"\n",
    "    try:\n",
    "        model = get_embedding_model()\n",
    "        if model is None:\n",
    "            return []\n",
    "            \n",
    "        # Encode the text to get embeddings\n",
    "        embedding = model.encode(text)\n",
    "        return embedding.tolist()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "def compute_similarity(embedding1: List[float], embedding2: List[float]) -> float:\n",
    "    \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "    if not embedding1 or not embedding2:\n",
    "        return 0.0\n",
    "        \n",
    "    try:\n",
    "        return cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing similarity: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# ======== LLM Query Function ========\n",
    "\n",
    "def query_huggingface(\n",
    "    prompt: str, \n",
    "    system_prompt: str = \"\", \n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Query local HuggingFace model with the given prompt.\"\"\"\n",
    "    try:\n",
    "        # Fall back to Hugging Face Inference API if available\n",
    "        if os.getenv(\"HF_API_TOKEN\"):\n",
    "            return query_hf_inference_api(prompt, system_prompt, max_tokens, temperature)\n",
    "            \n",
    "        tokenizer, model = get_llm_model()\n",
    "        if tokenizer is None or model is None:\n",
    "            return \"Model loading failed. Please check logs.\"\n",
    "            \n",
    "        # Format input for model\n",
    "        if system_prompt:\n",
    "            input_text = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        else:\n",
    "            input_text = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "            \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            # Move to GPU if available\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate text\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        # Decode the generated text\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract assistant's response\n",
    "        match = re.search(r\"<\\|im_start\\|>assistant\\n(.*?)(?:<\\|im_end\\|>|$)\", output, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return output.split(\"<|im_start|>assistant\\n\")[-1].strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying local model: {e}\")\n",
    "        fallback_message = f\"Error using local model: {str(e)}. Attempting to use Hugging Face Inference API...\"\n",
    "        logger.info(fallback_message)\n",
    "        \n",
    "        # Try using a simpler approach with a fallback prompt template\n",
    "        try:\n",
    "            tokenizer, model = get_llm_model()\n",
    "            if tokenizer and model:\n",
    "                # Simplified prompt template\n",
    "                simple_prompt = f\"{system_prompt}\\n\\n{prompt}\" if system_prompt else prompt\n",
    "                inputs = tokenizer(simple_prompt, return_tensors=\"pt\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model.to(device)\n",
    "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                    output_ids = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_tokens,\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "                \n",
    "                return tokenizer.decode(output_ids[0], skip_special_tokens=True)[len(simple_prompt):]\n",
    "        except Exception as nested_e:\n",
    "            logger.error(f\"Error with simplified generation: {nested_e}\")\n",
    "        \n",
    "        # Fall back to mock response for testing or try HF Inference API\n",
    "        if os.getenv(\"HF_API_TOKEN\"):\n",
    "            return query_hf_inference_api(prompt, system_prompt, max_tokens, temperature)\n",
    "        \n",
    "        # Last resort - generate a basic mock response for testing\n",
    "        if \"project\" in prompt.lower() and \"subproject\" in system_prompt.lower():\n",
    "            return '[{\"name\": \"Frontend\", \"description\": \"User interface\", \"tech_requirements\": [\"React\"], \"dependencies\": []}]'\n",
    "        \n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "def query_hf_inference_api(\n",
    "    prompt: str,\n",
    "    system_prompt: str = \"\",\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.7\n",
    ") -> str:\n",
    "    \"\"\"Query Hugging Face Inference API as a fallback.\"\"\"\n",
    "    try:\n",
    "        API_URL = \"https://api-inference.huggingface.co/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        headers = {\"Authorization\": f\"Bearer {os.getenv('HF_API_TOKEN')}\"}\n",
    "        \n",
    "        # Format the prompt\n",
    "        if system_prompt:\n",
    "            payload = {\n",
    "                \"inputs\": f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            payload = {\n",
    "                \"inputs\": f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[0].get(\"generated_text\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error using HF Inference API: {e}\")\n",
    "        return f\"Error with Inference API: {str(e)}\"\n",
    "\n",
    "# ======== Project Decomposition ========\n",
    "\n",
    "def decompose_project(project_description: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Break down project into subprojects using LLM.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical architect. Break down the given project into logical subprojects.\n",
    "    For each subproject, provide:\n",
    "    1. Name\n",
    "    2. Description\n",
    "    3. Technical requirements \n",
    "    4. Dependencies on other subprojects\n",
    "    \n",
    "    Format your response as a JSON array of objects with the structure:\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"tech_requirements\": [\"string\"],\n",
    "            \"dependencies\": [\"string\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    Keep your response concise and ONLY return the JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"Project description: {project_description}\\n\\nBreak this down into logical subprojects:\"\n",
    "    \n",
    "    response = query_huggingface(prompt, system_prompt=system_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Find the JSON content in the response\n",
    "        json_content = response\n",
    "        if \"```json\" in response:\n",
    "            json_content = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response:\n",
    "            json_content = response.split(\"```\")[1].strip()\n",
    "        \n",
    "        # Some additional parsing for robustness\n",
    "        if json_content.startswith('[') and json_content.endswith(']'):\n",
    "            subprojects = json.loads(json_content)\n",
    "        else:\n",
    "            # Try to find JSON array in the string\n",
    "            matches = re.search(r'\\[.*?\\]', json_content, re.DOTALL)\n",
    "            if matches:\n",
    "                subprojects = json.loads(matches.group(0))\n",
    "            else:\n",
    "                # Fallback mock response for testing if everything fails\n",
    "                subprojects = [\n",
    "                    {\n",
    "                        \"name\": \"Frontend\",\n",
    "                        \"description\": \"User interface component\",\n",
    "                        \"tech_requirements\": [\"React\", \"TypeScript\"],\n",
    "                        \"dependencies\": []\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Backend API\",\n",
    "                        \"description\": \"Server-side logic and data processing\",\n",
    "                        \"tech_requirements\": [\"Python\", \"FastAPI\"],\n",
    "                        \"dependencies\": []\n",
    "                    }\n",
    "                ]\n",
    "                logger.warning(\"Using fallback mock response for project decomposition\")\n",
    "            \n",
    "        # Generate embeddings for each subproject\n",
    "        for subproject in subprojects:\n",
    "            subproject_text = f\"{subproject['name']} {subproject['description']} {' '.join(subproject['tech_requirements'])}\"\n",
    "            subproject[\"embedding\"] = get_embeddings(subproject_text)\n",
    "            \n",
    "        return subprojects\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing LLM response for project decomposition: {e}\")\n",
    "        logger.debug(f\"Response: {response}\")\n",
    "        \n",
    "        # Fallback for testing\n",
    "        return [\n",
    "            {\n",
    "                \"name\": \"Frontend\",\n",
    "                \"description\": \"User interface component\",\n",
    "                \"tech_requirements\": [\"React\", \"TypeScript\"],\n",
    "                \"dependencies\": [],\n",
    "                \"embedding\": get_embeddings(\"Frontend User interface component React TypeScript\")\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Backend API\",\n",
    "                \"description\": \"Server-side logic and data processing\",\n",
    "                \"tech_requirements\": [\"Python\", \"FastAPI\"],\n",
    "                \"dependencies\": [],\n",
    "                \"embedding\": get_embeddings(\"Backend API Server-side logic and data processing Python FastAPI\")\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# ======== GitHub Repository Search ========\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def search_github_repos(query: str, n: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search GitHub for repositories matching the query, with caching for efficiency.\"\"\"\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "    \n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"sort\": \"stars\",\n",
    "        \"order\": \"desc\",\n",
    "        \"per_page\": n\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Searching GitHub for: {query}\")\n",
    "        response = requests.get(f\"{GITHUB_API_URL}/search/repositories\", headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        repos = response.json().get(\"items\", [])\n",
    "        \n",
    "        result = []\n",
    "        for repo in repos:\n",
    "            # Get additional details about each repo\n",
    "            languages_url = repo.get(\"languages_url\")\n",
    "            languages = []\n",
    "            if languages_url:\n",
    "                lang_response = requests.get(languages_url, headers=headers)\n",
    "                if lang_response.status_code == 200:\n",
    "                    languages = list(lang_response.json().keys())\n",
    "            \n",
    "            # Get README content asynchronously to improve performance\n",
    "            readme_content = get_readme(repo[\"full_name\"], headers)\n",
    "            \n",
    "            # Create embedding for the repository\n",
    "            embedding_text = f\"{repo['name']} {repo['description'] or ''} {' '.join(repo.get('topics', []))}\"\n",
    "            \n",
    "            repo_info = {\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"full_name\": repo[\"full_name\"],\n",
    "                \"description\": repo[\"description\"] or \"\",\n",
    "                \"url\": repo[\"html_url\"],\n",
    "                \"stars\": repo[\"stargazers_count\"],\n",
    "                \"forks\": repo[\"forks_count\"],\n",
    "                \"languages\": languages,\n",
    "                \"license\": repo.get(\"license\", {}).get(\"spdx_id\") if repo.get(\"license\") else None,\n",
    "                \"topics\": repo.get(\"topics\", []),\n",
    "                \"readme_summary\": summarize_readme(readme_content),\n",
    "                #\"embedding\": get_embeddings(embedding_text)\n",
    "            }\n",
    "            result.append(repo_info)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching GitHub: {e}\")\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=64)\n",
    "def get_readme(repo_full_name: str, headers: Dict[str, str]) -> str:\n",
    "    \"\"\"Get README content for a repository with caching.\"\"\"\n",
    "    try:\n",
    "        # Try to get the default README\n",
    "        response = requests.get(\n",
    "            f\"{GITHUB_API_URL}/repos/{repo_full_name}/readme\",\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json().get(\"content\", \"\")\n",
    "            encoding = response.json().get(\"encoding\", \"\")\n",
    "            \n",
    "            if encoding == \"base64\" and content:\n",
    "                return base64.b64decode(content).decode('utf-8', errors='replace')\n",
    "        \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting README: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def summarize_readme(readme_content: str) -> str:\n",
    "    \"\"\"Summarize README content using LLM.\"\"\"\n",
    "    if not readme_content or len(readme_content) < 100:\n",
    "        return readme_content\n",
    "    \n",
    "    # Truncate very long READMEs to avoid token limits\n",
    "    if len(readme_content) > 4000:\n",
    "        readme_content = readme_content[:4000] + \"...\"\n",
    "    \n",
    "    system_prompt = \"Summarize the following README in 2-3 sentences, focusing on what the repository does and its key features:\"\n",
    "    \n",
    "    return query_huggingface(readme_content, system_prompt=system_prompt, max_tokens=200)\n",
    "\n",
    "def find_repos_for_subproject(subproject: Dict[str, Any], n: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Find repositories for a specific subproject, with efficient search query construction.\"\"\"\n",
    "    # Construct optimized search query from subproject details\n",
    "    search_terms = [\n",
    "        subproject[\"name\"],\n",
    "        *[req for req in subproject[\"tech_requirements\"] if len(req.split()) <= 3]  # Only include short tech requirements\n",
    "    ]\n",
    "    \n",
    "    # Use NLP to extract key terms from description\n",
    "    if subproject[\"description\"]:\n",
    "        # Simple keyword extraction - in production you might use NLP techniques\n",
    "        stop_words = {\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \"and\", \"or\", \"of\"}\n",
    "        desc_words = [word.lower() for word in re.findall(r'\\w+', subproject[\"description\"]) \n",
    "                      if word.lower() not in stop_words and len(word) > 3]\n",
    "        \n",
    "        # Take most important keywords (longer words tend to be more specific)\n",
    "        desc_words.sort(key=len, reverse=True)\n",
    "        search_terms.extend(desc_words[:3])\n",
    "    \n",
    "    # Make query unique by removing duplicates while preserving order\n",
    "    seen = set()\n",
    "    query_terms = [term for term in search_terms if not (term.lower() in seen or seen.add(term.lower()))]\n",
    "    \n",
    "    # Join terms and ensure we don't exceed GitHub's query length limits\n",
    "    query = \" \".join(query_terms)\n",
    "    if len(query) > 256:  # GitHub has query length limits\n",
    "        query = query[:256]\n",
    "    \n",
    "    return search_github_repos(query, n)\n",
    "\n",
    "# ======== Conflict Analysis ========\n",
    "\n",
    "def analyze_conflicts(subprojects: List[Dict[str, Any]], repos_by_subproject: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze potential integration conflicts between selected repositories.\"\"\"\n",
    "    conflicts = []\n",
    "    \n",
    "    # More comprehensive language compatibility matrix\n",
    "    compatible_languages = {\n",
    "        \"Python\": [\"Python\", \"JavaScript\", \"TypeScript\", \"C\", \"C++\", \"Rust\"],\n",
    "        \"JavaScript\": [\"JavaScript\", \"TypeScript\", \"Python\", \"HTML\", \"CSS\", \"Ruby\"],\n",
    "        \"TypeScript\": [\"TypeScript\", \"JavaScript\", \"Python\", \"HTML\", \"CSS\"],\n",
    "        \"Java\": [\"Java\", \"Kotlin\", \"Scala\", \"Groovy\"],\n",
    "        \"Kotlin\": [\"Kotlin\", \"Java\", \"Scala\"],\n",
    "        \"Go\": [\"Go\", \"C\", \"Rust\"],\n",
    "        \"Rust\": [\"Rust\", \"C\", \"C++\", \"Go\"],\n",
    "        \"C#\": [\"C#\", \"F#\", \"Visual Basic\", \"JavaScript\"],\n",
    "        \"PHP\": [\"PHP\", \"JavaScript\", \"HTML\", \"CSS\"],\n",
    "        \"Ruby\": [\"Ruby\", \"JavaScript\", \"HTML\", \"CSS\"],\n",
    "        \"Swift\": [\"Swift\", \"Objective-C\", \"C\"],\n",
    "        \"C++\": [\"C++\", \"C\", \"Rust\", \"Python\"],\n",
    "        \"C\": [\"C\", \"C++\", \"Rust\", \"Assembly\"]\n",
    "    }\n",
    "    \n",
    "    # More comprehensive license compatibility matrix\n",
    "    # This is simplified - real license compatibility is complex and should be evaluated by legal professionals\n",
    "    license_compatibility = {\n",
    "        \"AGPL-3.0\": {\n",
    "            \"incompatible\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\"],\n",
    "            \"compatible\": [\"GPL-3.0\", \"LGPL-3.0\"]\n",
    "        },\n",
    "        \"GPL-3.0\": {\n",
    "            \"incompatible\": [\"MIT\", \"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\"],\n",
    "            \"compatible\": [\"LGPL-3.0\", \"AGPL-3.0\"]\n",
    "        },\n",
    "        \"LGPL-3.0\": {\n",
    "            \"incompatible\": [],\n",
    "            \"compatible\": [\"GPL-3.0\", \"AGPL-3.0\", \"MIT\", \"Apache-2.0\", \"BSD-3-Clause\"]\n",
    "        },\n",
    "        \"MIT\": {\n",
    "            \"incompatible\": [\"AGPL-3.0\", \"GPL-3.0\"],\n",
    "            \"compatible\": [\"Apache-2.0\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\", \"LGPL-3.0\"]\n",
    "        },\n",
    "        \"Apache-2.0\": {\n",
    "            \"incompatible\": [\"AGPL-3.0\", \"GPL-3.0\"],\n",
    "            \"compatible\": [\"MIT\", \"BSD-3-Clause\", \"BSD-2-Clause\", \"MPL-2.0\", \"LGPL-3.0\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check for language conflicts\n",
    "    for i, subproject1 in enumerate(subprojects):\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            languages1 = repo1.get(\"languages\", [])\n",
    "            primary_lang1 = languages1[0] if languages1 else None\n",
    "            \n",
    "            for j, subproject2 in enumerate(subprojects):\n",
    "                if i == j:\n",
    "                    continue  # Skip self-comparison\n",
    "                    \n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if subproject2 depends on subproject1 or vice versa\n",
    "                has_dependency = (name1 in subproject2.get(\"dependencies\", []) or\n",
    "                                 name2 in subproject1.get(\"dependencies\", []))\n",
    "                \n",
    "                if not has_dependency:\n",
    "                    continue  # Only check conflicts for dependent components\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    languages2 = repo2.get(\"languages\", [])\n",
    "                    primary_lang2 = languages2[0] if languages2 else None\n",
    "                    \n",
    "                    # Check language compatibility\n",
    "                    if (primary_lang1 and primary_lang2 and \n",
    "                        primary_lang1 in compatible_languages and\n",
    "                        primary_lang2 not in compatible_languages.get(primary_lang1, [])):\n",
    "                        conflicts.append({\n",
    "                            \"type\": \"language_incompatibility\",\n",
    "                            \"severity\": \"high\",\n",
    "                            \"description\": f\"Language incompatibility between {name1} ({primary_lang1}) and {name2} ({primary_lang2})\",\n",
    "                            \"subprojects\": [name1, name2],\n",
    "                            \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]],\n",
    "                            \"mitigation\": f\"Consider using language bindings, APIs, or microservices to integrate {primary_lang1} with {primary_lang2}\"\n",
    "                        })\n",
    "    \n",
    "    # Check for license conflicts\n",
    "    for subproject1 in subprojects:\n",
    "        name1 = subproject1[\"name\"]\n",
    "        if name1 not in repos_by_subproject:\n",
    "            continue\n",
    "            \n",
    "        for repo1 in repos_by_subproject[name1]:\n",
    "            license1 = repo1.get(\"license\")\n",
    "            \n",
    "            for subproject2 in subprojects:\n",
    "                name2 = subproject2[\"name\"]\n",
    "                if name2 == name1 or name2 not in repos_by_subproject:\n",
    "                    continue\n",
    "                    \n",
    "                for repo2 in repos_by_subproject[name2]:\n",
    "                    license2 = repo2.get(\"license\")\n",
    "                    \n",
    "                    # Check license compatibility\n",
    "                    if (license1 and license2 and \n",
    "                        license1 in license_compatibility and\n",
    "                        license2 in license_compatibility.get(license1, {}).get(\"incompatible\", [])):\n",
    "                        conflicts.append({\n",
    "                            \"type\": \"license_incompatibility\",\n",
    "                            \"severity\": \"high\",\n",
    "                            \"description\": f\"License incompatibility between {name1} ({license1}) and {name2} ({license2})\",\n",
    "                            \"subprojects\": [name1, name2],\n",
    "                            \"repos\": [repo1[\"full_name\"], repo2[\"full_name\"]],\n",
    "                            \"mitigation\": \"Consider finding alternative repositories with compatible licenses or seeking legal advice\"\n",
    "                        })\n",
    "    \n",
    "    return conflicts\n",
    "\n",
    "# ======== Integration Planning ========\n",
    "\n",
    "def generate_integration_plan(\n",
    "    project_description: str,\n",
    "    subprojects: List[Dict[str, Any]],\n",
    "    selected_repos: Dict[str, Dict[str, Any]],\n",
    "    conflicts: List[Dict[str, Any]]\n",
    ") -> str:\n",
    "    \"\"\"Generate an integration plan using LLM with improved prompting.\"\"\"\n",
    "    # Prepare a structured context for the LLM\n",
    "    subproject_summaries = []\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repo = selected_repos.get(name, {})\n",
    "        \n",
    "        # Build a comprehensive description of the subproject and selected repo\n",
    "        summary = {\n",
    "            \"name\": name,\n",
    "            \"description\": subproject[\"description\"],\n",
    "            \"tech_requirements\": subproject[\"tech_requirements\"],\n",
    "            \"dependencies\": subproject.get(\"dependencies\", [])\n",
    "        }\n",
    "        \n",
    "        if repo:\n",
    "            summary[\"repository\"] = {\n",
    "                \"name\": repo[\"name\"],\n",
    "                \"full_name\": repo[\"full_name\"],\n",
    "                \"url\": repo.get(\"url\", \"\"),\n",
    "                \"description\": repo.get(\"description\", \"\"),\n",
    "                \"primary_language\": repo.get(\"languages\", [\"\"])[0] if repo.get(\"languages\") else \"\",\n",
    "                \"stars\": repo.get(\"stars\", 0)\n",
    "            }\n",
    "            \n",
    "        subproject_summaries.append(summary)\n",
    "    \n",
    "    # Format conflict information\n",
    "    conflict_details = []\n",
    "    for conflict in conflicts:\n",
    "        conflict_details.append({\n",
    "            \"type\": conflict[\"type\"],\n",
    "            \"description\": conflict[\"description\"],\n",
    "            \"affected_components\": conflict[\"subprojects\"],\n",
    "            \"mitigation\": conflict.get(\"mitigation\", \"Needs mitigation strategy\")\n",
    "        })\n",
    "    \n",
    "    # Create structured input for the LLM\n",
    "    plan_input = {\n",
    "        \"project_description\": project_description,\n",
    "        \"subprojects\": subproject_summaries,\n",
    "        \"conflicts\": conflict_details\n",
    "    }\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical integration architect with expertise in software development and system design.\n",
    "    Create a practical integration plan for connecting the selected repositories into a cohesive system.\n",
    "    \n",
    "    Your plan should include:\n",
    "    1. Architecture overview - how components will fit together\n",
    "    2. Key integration points between components\n",
    "    3. Specific modifications needed for each repository\n",
    "    4. Step-by-step implementation plan\n",
    "    5. Solutions for the identified conflicts\n",
    "    \n",
    "    Focus on practical implementation details rather than theory.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    I need an integration plan for the following software project:\n",
    "    \n",
    "    {json.dumps(plan_input, indent=2)}\n",
    "    \n",
    "    Please provide a comprehensive integration plan that covers architecture, \n",
    "    integration points, necessary modifications, and conflict resolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_huggingface(prompt, system_prompt=system_prompt, max_tokens=1500)\n",
    "\n",
    "# ======== Semantic Matching Functions ========\n",
    "\n",
    "def rank_repos_by_similarity(subproject: Dict[str, Any], repos: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Rank repositories by semantic similarity to subproject with improved algorithm.\"\"\"\n",
    "    if not repos or not subproject.get(\"embedding\") or len(subproject[\"embedding\"]) == 0:\n",
    "        return repos\n",
    "        \n",
    "    # Calculate multiple similarity factors\n",
    "    results = []\n",
    "    for repo in repos:\n",
    "        if not repo.get(\"embedding\") or len(repo[\"embedding\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        semantic_similarity = compute_similarity(subproject[\"embedding\"], repo[\"embedding\"])\n",
    "        \n",
    "        # Calculate tech requirement match score \n",
    "        tech_match_score = 0\n",
    "        if repo.get(\"topics\") and subproject.get(\"tech_requirements\"):\n",
    "            # Case-insensitive matching of tech requirements with repo topics\n",
    "            repo_topics_lower = [topic.lower() for topic in repo[\"topics\"]]\n",
    "            matched_techs = sum(1 for req in subproject[\"tech_requirements\"] \n",
    "                               if any(req.lower() in topic for topic in repo_topics_lower))\n",
    "            tech_match_score = matched_techs / len(subproject[\"tech_requirements\"]) if subproject[\"tech_requirements\"] else 0\n",
    "        \n",
    "        # Factor in repository popularity (stars) as a quality signal\n",
    "        popularity_score = min(1.0, repo.get(\"stars\", 0) / 10000)  # Cap at 10k stars for normalization\n",
    "        \n",
    "        # Combined score with weights \n",
    "        combined_score = (\n",
    "            0.5 * semantic_similarity +  # Semantic similarity is most important\n",
    "            0.3 * tech_match_score +     # Technical match is important\n",
    "            0.2 * popularity_score       # Popularity provides some quality assurance\n",
    "        )\n",
    "        \n",
    "        results.append((combined_score, repo))\n",
    "    \n",
    "    # Sort by combined score\n",
    "    results.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Return sorted repositories\n",
    "    return [repo for _, repo in results]\n",
    "\n",
    "# ======== Main Execution Function ========\n",
    "\n",
    "def create_project_plan(project_description: str) -> Dict[str, Any]:\n",
    "    \"\"\"Create a complete project plan from description with improved execution flow.\"\"\"\n",
    "    logger.info(\"Starting project planning process\")\n",
    "    \n",
    "    # Step 1: Decompose project into subprojects\n",
    "    logger.info(\"Decomposing project...\")\n",
    "    subprojects = decompose_project(project_description)\n",
    "    if not subprojects:\n",
    "        return {\"error\": \"Failed to decompose project\"}\n",
    "    \n",
    "    # Step 2: Find repositories for each subproject\n",
    "    logger.info(\"Searching repositories...\")\n",
    "    repos_by_subproject = {}\n",
    "    for subproject in subprojects:\n",
    "        name = subproject[\"name\"]\n",
    "        repos = find_repos_for_subproject(subproject, n=5)  # Get top 5 repos\n",
    "        if repos:\n",
    "            # Rank repos by similarity\n",
    "            ranked_repos = rank_repos_by_similarity(subproject, repos)\n",
    "            repos_by_subproject[name] = ranked_repos\n",
    "    \n",
    "    # Step 3: Select best repository for each subproject\n",
    "    selected_repos = {}\n",
    "    for name, repos in repos_by_subproject.items():\n",
    "        if repos:\n",
    "            selected_repos[name] = repos[0]  # Select highest ranked repo\n",
    "    \n",
    "    # Step 4: Analyze conflicts\n",
    "    logger.info(\"Analyzing potential conflicts...\")\n",
    "    conflicts = analyze_conflicts(subprojects, repos_by_subproject)\n",
    "    \n",
    "    # Step 5: Generate integration plan\n",
    "    logger.info(\"Generating integration plan...\")\n",
    "    integration_plan = generate_integration_plan(\n",
    "        project_description, \n",
    "        subprojects, \n",
    "        selected_repos,\n",
    "        conflicts\n",
    "    )\n",
    "    \n",
    "    # Build complete result\n",
    "    return {\n",
    "        \"project_description\": project_description,\n",
    "        \"subprojects\": subprojects,\n",
    "        \"selected_repositories\": selected_repos,\n",
    "        \"alternative_repositories\": repos_by_subproject,\n",
    "        \"conflicts\": conflicts,\n",
    "        \"integration_plan\": integration_plan,\n",
    "        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# ======== Demo Usage ========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example project description\n",
    "    project_description = \"\"\"\n",
    "    Build a web application that allows users to upload images, \n",
    "    apply ML-based filters, and share them on social media.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run the planner\n",
    "    #plan = create_project_plan(project_description)\n",
    "    \n",
    "    # Output the plan (could be saved to file or displayed in a UI)\n",
    "    #print(json.dumps(plan, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcaf61-6325-4ff2-90e9-52633f7c2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "# Add the directory containing project_planner.py to the Python path\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\".\")))\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    \"\"\"Render the main page.\"\"\"\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/plan', methods=['POST'])\n",
    "def generate_project_plan():\n",
    "    \"\"\"Generate project plan endpoint.\"\"\"\n",
    "    try:\n",
    "        # Get project description from form\n",
    "        project_description = request.form.get('project_description', '').strip()\n",
    "        \n",
    "        # Validate input\n",
    "        if not project_description:\n",
    "            return jsonify({\n",
    "                'error': 'Please provide a project description.',\n",
    "                'status': 'error'\n",
    "            }), 400\n",
    "        \n",
    "        # Generate project plan\n",
    "        plan = create_project_plan(project_description)\n",
    "        \n",
    "        # Render plan template for HTMX partial response\n",
    "        return render_template('project_plan.html', plan=plan)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log the error (in production, use proper logging)\n",
    "        print(f\"Error generating project plan: {e}\")\n",
    "        return jsonify({\n",
    "            'error': 'An error occurred while generating the project plan.',\n",
    "            'details': str(e),\n",
    "            'status': 'error'\n",
    "        }), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure environment variables are set for GitHub/HuggingFace tokens if needed\n",
    "    # os.environ['GITHUB_TOKEN'] = 'your_github_token'\n",
    "    # os.environ['HF_API_TOKEN'] = 'your_huggingface_token'\n",
    "    \n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6d8f48-40ad-4ff3-8b83-b6f156b679f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_project(project_description: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Break down project into subprojects using LLM.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are a technical architect. Break down the given project into logical subprojects.\n",
    "    For each subproject, provide:\n",
    "    1. Name\n",
    "    2. Description\n",
    "    3. Technical requirements \n",
    "    4. Dependencies on other subprojects\n",
    "    \n",
    "    Format your response as a JSON array of objects with the structure:\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"tech_requirements\": [\"string\"],\n",
    "            \"dependencies\": [\"string\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    Keep your response concise and ONLY return the JSON array.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"Project description: {project_description}\\n\\n I am trying to breakdown the project into multiple mini projects available on github, Break this down into logical subprojects, and justify ur reasoning.\"\n",
    "    \n",
    "    response = query_huggingface(prompt, system_prompt=system_prompt)\n",
    "    print(response)\n",
    "    try:\n",
    "        # Find the JSON content in the response\n",
    "        json_content = response\n",
    "        if \"```json\" in response:\n",
    "            json_content = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response:\n",
    "            json_content = response.split(\"```\")[1].strip()\n",
    "        \n",
    "        # Some additional parsing for robustness\n",
    "        if json_content.startswith('[') and json_content.endswith(']'):\n",
    "            subprojects = json.loads(json_content)\n",
    "        else:\n",
    "            # Try to find JSON array in the string\n",
    "            matches = re.search(r'\\[.*?\\]', json_content, re.DOTALL)\n",
    "            if matches:\n",
    "                subprojects = json.loads(matches.group(0))\n",
    "            else:\n",
    "                # Fallback mock response for testing if everything fails\n",
    "                subprojects = [\n",
    "                    {\n",
    "                        \"name\": \"Frontend\",\n",
    "                        \"description\": \"User interface component\",\n",
    "                        \"tech_requirements\": [\"React\", \"TypeScript\"],\n",
    "                        \"dependencies\": []\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Backend API\",\n",
    "                        \"description\": \"Server-side logic and data processing\",\n",
    "                        \"tech_requirements\": [\"Python\", \"FastAPI\"],\n",
    "                        \"dependencies\": []\n",
    "                    }\n",
    "                ]\n",
    "                logger.warning(\"Using fallback mock response for project decomposition\")\n",
    "            \n",
    "        # Generate embeddings for each subproject\n",
    "        for subproject in subprojects:\n",
    "            subproject_text = f\"{subproject['name']} {subproject['description']} {' '.join(subproject['tech_requirements'])}\"\n",
    "            subproject[\"embedding\"] = get_embeddings(subproject_text)\n",
    "            \n",
    "        return subprojects\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing LLM response for project decomposition: {e}\")\n",
    "        logger.debug(f\"Response: {response}\")\n",
    "        \n",
    "        # Fallback for testing\n",
    "        return [\n",
    "            {\n",
    "                \"name\": \"Frontend\",\n",
    "                \"description\": \"User interface component\",\n",
    "                \"tech_requirements\": [\"React\", \"TypeScript\"],\n",
    "                \"dependencies\": [],\n",
    "                \"embedding\": get_embeddings(\"Frontend User interface component React TypeScript\")\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Backend API\",\n",
    "                \"description\": \"Server-side logic and data processing\",\n",
    "                \"tech_requirements\": [\"Python\", \"FastAPI\"],\n",
    "                \"dependencies\": [],\n",
    "                \"embedding\": get_embeddings(\"Backend API Server-side logic and data processing Python FastAPI\")\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b5826-be67-486a-91f1-565fdd533aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
